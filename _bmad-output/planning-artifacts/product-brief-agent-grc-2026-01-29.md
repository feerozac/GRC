---
stepsCompleted: [1, 2]
inputDocuments: []
date: 2026-01-29
author: Mark
---

# Product Brief: agent-grc

<!-- Content will be appended sequentially through collaborative workflow steps -->

## Executive Summary

Agentic AI GRC is an enterprise governance, risk, and compliance platform built to eliminate the manual, time‑intensive work of remediation, compliance, regulatory approvals, control testing, and risk identification. It embeds AI agents with distinct personas aligned to the three lines of defence (1L/2L/3L) so governance stays fully engaged while repetitive work is automated. The result is faster decision cycles, audit‑ready outcomes, and regulator‑friendly accountability without sacrificing human oversight.

---

## Core Vision

### Problem Statement

Managing remediation, compliance, regulatory approvals, control testing, risk identification, and strategy linkage is labor‑intensive and fragmented. Risk and compliance teams are highly specialised and costly, yet still spend too much time on coordination rather than reducing risk.

### Problem Impact

Audit preparation remains slow and reactive, regulatory change outpaces reporting cycles, and accountability is hard to evidence. This increases operational risk, audit pressure, and compliance cost.

### Why Existing Solutions Fall Short

Traditional GRC tools organise workflows but do not operate governance continuously. They automate tasks but cannot provide real‑time governance engagement, explainability at scale, or strategy‑to‑metrics translation without heavy manual effort.

### Proposed Solution

An agentic AI–enabled enterprise GRC platform that automates the work while preserving accountability. Human‑in‑the‑loop checkpoints ensure high‑risk decisions are validated by experts, while AI agents continuously identify risks, accelerate remediation, interpret regulatory change, and map governance decisions back to business strategy. This creates audit‑proof, regulator‑ready outputs without slowing the organisation.

### Key Differentiators

- **Accountability by design:** Humans own high‑risk decisions; AI actions are traceable and auditable.
- **Explainability at scale:** Plain‑language executive narratives plus technical rationale for auditors.
- **Automation without loss of rigor:** Repetitive work automated, assurance strengthened by continuous evidence.
- **Strategy → governance “round‑trip”:** Strategy and board intent translate to objectives, policies, standards, org/process, metrics, and logs.
- **3LOD‑aligned agent personas:** Agents reinforce governance culture rather than bypass it.

---

## Competitive Analysis

### Evaluation Criteria (Weighted)

| Criteria | Weight | Description |
|----------|--------|-------------|
| **Agentic AI Capability** | 25% | True autonomous agents vs workflow automation |
| **3LOD Native Design** | 20% | First-class 1L/2L/3L separation and workflows |
| **Explainability at Scale** | 15% | Plain-language + technical audit rationale |
| **Strategy-to-Metrics Round-Trip** | 15% | Board intent → objectives → evidence → logs |
| **APAC Regulatory Intelligence** | 10% | HKMA/MAS near real-time coverage |
| **Org/Doc Governance Mapping** | 10% | Org chart + annual report ingestion |
| **Time to Audit-Ready** | 5% | Speed of board pack generation |

### Competitor Scoring (1-5 scale)

| Vendor | Agentic AI (25%) | 3LOD (20%) | Explainability (15%) | Strategy→Metrics (15%) | APAC Reg (10%) | Org/Doc (10%) | Audit Speed (5%) | **Weighted Score** |
|--------|------------------|------------|---------------------|------------------------|----------------|---------------|------------------|-------------------|
| **Agentic AI GRC (Ours)** | 5 | 5 | 5 | 5 | 5 | 5 | 5 | **5.00** |
| IBM OpenPages | 4 | 3 | 3 | 2 | 3 | 2 | 3 | **2.95** |
| ServiceNow GRC | 4 | 2 | 3 | 2 | 2 | 2 | 3 | **2.65** |
| AuditBoard | 3 | 3 | 4 | 2 | 2 | 2 | 4 | **2.80** |
| MetricStream | 3 | 3 | 3 | 2 | 4 | 2 | 3 | **2.80** |
| Diligent | 3 | 2 | 3 | 3 | 2 | 2 | 4 | **2.65** |

### Why Competitors Don't Cover Org/Doc Governance Mapping

| Barrier | Incumbent Challenge | Our Advantage |
|---------|---------------------|---------------|
| **Architecture Legacy** | Built 10-15 years ago as structured data systems; retrofitting NLP/ML is expensive | Built agent-first with document intelligence native |
| **Data Model Limitations** | Model Risks → Controls → Evidence only; no strategy-to-metrics lineage | Governance exoskeleton: Strategy → Objectives → Policies → Standards → Org → Process → Metrics → Logs |
| **Customer Expectation Mismatch** | Optimized for "help me pass audits" not "interpret my board papers" | Designed for strategy-to-governance translation |
| **AI Maturity Gap** | Experimenting with LLMs; none shipped org/doc parsing at scale | Production-ready with human-in-the-loop validation |
| **Liability Concerns** | Risk-averse about AI-generated governance recommendations | Explainability by Design + immutable audit trail |

### Primary Differentiators to Lead With

1. **Agentic-native architecture** — not retrofitted AI on legacy workflows
2. **First-class 3LOD** — persona-aligned agents preserving governance culture
3. **Strategy → metrics round-trip** — governance exoskeleton from board to logs
4. **APAC regulatory intelligence** — HKMA/MAS 24h SLA

### Competitive Positioning

Position as the **only agent-first GRC platform** built for continuous governance, not workflow automation with AI bolted on.

---

## Liability & Accountability Framework

Agentic AI GRC is designed for **accountability by design**, not liability transfer. AI outputs are recommendations, not decisions. Every material action requires human approval from a role-appropriate reviewer, with mandatory explainability and an immutable audit trail. Contractual terms are explicit: the AI assists, the human decides, the customer owns the outcome. This model aligns with emerging regulatory expectations (HKMA, MAS, EU AI Act) for AI in governance.

---

## Explainability Framework Alignment

Agentic AI GRC implements a **6-layer explainability framework** aligned to international standards:

### Standards Alignment

| Standard | Status | Relevance |
|----------|--------|-----------|
| **NIST AI RMF** | Voluntary; US baseline | Govern → Map → Measure → Manage structure |
| **ISO/IEC 42001:2023** | Certifiable | AI Management System; PDCA audit trail |
| **EU AI Act** | Legally binding (EU) | Transparency & human oversight (Art. 13, 14) |
| **MAS AI Risk Guidelines** | Final 2026 | APAC financial services lifecycle controls |
| **HKMA GenAI Principles** | Active | Hong Kong market alignment |

### 6-Layer Explainability Model

| Layer | Requirement | Framework Alignment |
|-------|-------------|---------------------|
| **1. Decision Transparency** | Every AI recommendation includes: sources, confidence score, reasoning chain | NIST AI RMF, EU AI Act Art. 13 |
| **2. Audit Trail Integrity** | Immutable, tamper-evident log of all AI outputs + human approvals | ISO/IEC 42001, MAS lifecycle controls |
| **3. Role-Appropriate Review** | High-risk outputs require approval by qualified reviewer (not rubber-stamp) | MAS human oversight, EU AI Act Art. 14 |
| **4. Confidence Thresholds** | Low-confidence outputs flagged for mandatory human drafting | MAS proportionate approach |
| **5. Regulator-Ready Reports** | On-demand explainability reports: plain language (board) + technical (auditor) | NIST Playbook, EU AI Act transparency |
| **6. Continuous Monitoring** | Drift detection, periodic human-only audits, bias checks | ISO/IEC 42001 PDCA, MAS evaluation & testing |

### Certification Roadmap

| Milestone | Timeframe | Benefit |
|-----------|-----------|---------|
| **NIST AI RMF alignment** | Launch | US credibility, foundation for ISO |
| **ISO/IEC 42001 certification** | Year 1 | International credibility, audit-ready |
| **EU AI Act compliance** | Year 1-2 | EU market access |
| **MAS/HKMA sandbox participation** | Year 1 | APAC regulator relationship |

---

## Go-to-Market Trust Strategy

Enterprise buyers (CISOs, CROs) are risk-averse. "Nobody got fired for buying IBM" is a real barrier. Our trust-building strategy:

| Tactic | Implementation | Outcome |
|--------|----------------|---------|
| **Regulator endorsement** | Participate in MAS/HKMA GenAI Sandboxes | "Regulator-vetted" positioning |
| **Big 4 partnership** | Strategic alliance with Deloitte, KPMG, or PwC for implementation | Trust transfer via established brand |
| **ISO/IEC 42001 certification** | Independent AI governance audit | Buyers can point to certification |
| **Cyber insurance backing** | Partner with insurer to offer "AI GRC coverage" add-on | Transfers perceived risk |
| **Pilot-first sales model** | 90-day paid pilot with defined success metrics | Low-risk entry for buyers |
| **Case study velocity** | First 3 customers get discounted rates for public case studies | Social proof acceleration |

---

## Competitive Moat

### Why Incumbents Can't Copy Quickly

| Barrier | Incumbent Challenge | Time to Overcome |
|---------|---------------------|------------------|
| **Architecture debt** | ServiceNow/IBM built 10-15 years ago as workflow engines; agent-first requires core rearchitecture | 2-3 years |
| **Data model lock-in** | Existing Risk → Control → Evidence schema; strategy-to-metrics lineage breaks deployments | 18+ months |
| **Talent gap** | LLM/agent engineering ≠ enterprise workflow dev; hiring + culture integration slow | 12-18 months |
| **Customer expectation anchor** | Customers bought "audit workflow"; pivot to "governance intelligence" confuses positioning | 12+ months |
| **Cannibalization fear** | AI automating GRC work threatens professional services revenue; incentivized to go slow | Indefinite |

### Time-to-Moat Calculation

| Capability | Defensible Lead |
|------------|-----------------|
| Agent-first architecture | 18+ months |
| 3LOD persona engine | 12+ months |
| Strategy-to-metrics lineage | 18+ months |
| APAC regulatory feed (24h SLA) | 12+ months |
| **Total estimated lead** | **18-24 months** |

---

## Go-to-Market Economics

Enterprise GRC deals take 9-18 months. Our strategy to reduce cash-to-close risk:

### Revenue Model

| Lever | Implementation | Benefit |
|-------|----------------|---------|
| **Mid-market wedge** | Target 500-2000 employee firms first | Faster cycles (3-6 months) |
| **PLG component** | Free "Governance Health Check" tool (upload annual report → gap analysis) | Lead generation + upsell |
| **Usage-based pricing** | Lower entry point + expansion as agents/users/documents scale | Land-and-expand |
| **Channel leverage** | Big 4 / regional consultancies sell and implement | Platform fee + services split |
| **APAC focus** | Smaller market but less competition | Faster regulator relationships |

### Runway Protection

| Scenario | Response |
|----------|----------|
| First 2 enterprise deals slip | Mid-market pipeline provides 6+ smaller deals to bridge |
| Burn exceeds plan | Reduce R&D; core platform stable, prioritize sales engineering |
| Competitor fast-follow | Double down on APAC; incumbents won't prioritize HK/SG first |

---

## Proof of Differentiation

### The "Cannot Do" Test

What can Agentic AI GRC do that IBM OpenPages literally cannot do today?

| Capability | IBM OpenPages | Agentic AI GRC |
|------------|---------------|----------------|
| Ingest annual report → auto-generate governance objectives | ❌ No document AI | ✅ Native |
| Map org chart to process ownership suggestions | ❌ Manual entry only | ✅ AI-proposed |
| Real-time regulatory change → auto-gap analysis (24h SLA) | ❌ Quarterly manual updates | ✅ HKMA/MAS feeds |
| 3LOD-persona agents with distinct challenge/audit behaviors | ❌ Single user role model | ✅ 1L/2L/3L personas |
| Strategy statement → KCI/KRI linkage with confidence scores | ❌ No strategy ingestion | ✅ Round-trip lineage |
| Adversarial control testing by AI red team | ❌ No adversarial capability | ✅ Built-in |

### The 60-Second Demo

> "Upload your annual report. In 60 seconds, we'll show you:
> 1. Extracted strategic priorities
> 2. Mapped governance objectives
> 3. Suggested KCIs with confidence scores
> 4. Gap analysis against HKMA guidelines
>
> IBM can't do step 1."

This demo is the proof point for every sales conversation.

---

## Risk Summary (Shark Tank Stress Test)

| Challenge | Risk | Mitigation |
|-----------|------|------------|
| **Enterprise trust** | CISOs won't risk career on unproven startup | Regulator sandbox + Big 4 + ISO cert + insurance |
| **AI liability** | Who's liable when AI gets it wrong? | Accountability by design; human decides, AI assists |
| **Incumbent fast-follow** | ServiceNow ships in 6 months | 18-24 month moat; architecture debt |
| **Unit economics** | Long sales cycles burn cash | Mid-market wedge + PLG + channel + APAC focus |
| **Differentiation proof** | "Agent-first" is just buzzwords | 60-second annual report demo |
